{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Chithra\n",
      "[nltk_data]     Menon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Chithra\n",
      "[nltk_data]     Menon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import time\n",
    "import nltk\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##dir_path = os.path.abspath(os.path.dirname(__file__)) \n",
    "##input_path = os.path.join(dir_path,'Dataset')\n",
    "\n",
    "#Get Russian Troll Data from CSV file\n",
    "#combine all the csv files into one large csv file of approx 3M rows\n",
    "path1 = r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls'\n",
    "all_trollfiles = glob.glob(os.path.join(path1, \"*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, dtype='unicode') for f in all_trollfiles)\n",
    "df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "\n",
    "#Get Non Russian Troll Data from excel\n",
    "random_df1 = pd.read_csv(r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls\\RandomTweets\\RandomTweets1.csv', usecols=['content', 'Country', 'Tweet language (ISO 639-1)'])\n",
    "random_df2 = pd.read_csv(r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls\\RandomTweets\\RandomTweets2.csv', names = ['Sentiment', 'ID', 'publish_date', 'Flag', 'User', 'content'])\n",
    "random_df3 = pd.read_csv(r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls\\RandomTweets\\RandomTweets3.csv', usecols=['content'])\n",
    "random_df4 = pd.read_csv(r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls\\RandomTweets\\RandomTweets4.csv', usecols=['content'])\n",
    "random_df5 = pd.read_csv(r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls\\RandomTweets\\RandomTweets5.csv', usecols=['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2946207"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random_df2 = random_df2[['Sentiment', 'ID', 'publish_date', 'Flag', 'User', 'content']]\n",
    "\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336661\n"
     ]
    }
   ],
   "source": [
    "length = len(random_df1.index) + len(random_df2.index) + len(random_df3.index) + len(random_df4.index) + len(random_df5.index)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4400864"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean Up the seperate files\n",
    "\n",
    "#select necessary rows from the dataframe from troll data\n",
    "df = df[df['language'] == 'English']\n",
    "df['russian_troll'] = 1\n",
    "df_svm = df[['content', 'russian_troll']]\n",
    "\n",
    "#select tweets from US from random file1 and drop other columns\n",
    "random_df1 = random_df1[random_df1['Tweet language (ISO 639-1)'] == 'en']\n",
    "random_df1['russian_troll'] = 0\n",
    "random_df1_svm = random_df1[['content', 'russian_troll']]\n",
    "\n",
    "#select tweets from random file2 and drop other columns\n",
    "random_df2['russian_troll'] = 0\n",
    "random_df2_svm = random_df2[['content', 'russian_troll']]\n",
    "\n",
    "#select tweets from random file3\n",
    "random_df3['russian_troll'] = 0\n",
    "random_df3_svm = random_df3[['content', 'russian_troll']]\n",
    "\n",
    "#select tweets from random file4\n",
    "random_df4.columns = ['content']\n",
    "random_df4['russian_troll'] = 0\n",
    "random_df4_svm = random_df4[['content', 'russian_troll']]\n",
    "\n",
    "#Combine the three datasets\n",
    "svm_df1 = df_svm.append(random_df1_svm, ignore_index = True)\n",
    "svm_df2 = svm_df1.append(random_df2_svm, ignore_index = True)\n",
    "svm_df3 = svm_df2.append(random_df3_svm, ignore_index = True)\n",
    "svm_df = svm_df3.append(random_df4_svm, ignore_index = True)\n",
    "\n",
    "svm_df.head()\n",
    "len(svm_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the tweet content in svm_df\n",
    "\n",
    "#remove URLs from the tweet content\n",
    "svm_df['content'] = svm_df['content'].str.replace('https?:\\/\\/.*[\\r\\n]*', '')\n",
    "\n",
    "#remove non ASCII characters from tweets\n",
    "svm_df['content'] = svm_df['content'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "#remove words starting with @username as its not relevant to our classification\n",
    "svm_df['content'] = svm_df['content'].str.replace('@(.+?)[\\s,.;]', '')\n",
    "\n",
    "#remove words starting with & as they represent HTML character reference \n",
    "svm_df['content'] = svm_df['content'].str.replace('&(.+?)[\\s,.;]', '')\n",
    "\n",
    "#remove numerics and special characters except # from the string\n",
    "svm_df['content'] = svm_df['content'].str.replace('[^a-zA-Z#\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords from svm_df\n",
    "stop = stopwords.words('english')\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "\n",
    "svm_df['content_without_stopwords'] = svm_df['content'].str.replace(pat, '')\n",
    "svm_df['content_without_stopwords'] = svm_df['content_without_stopwords'].str.replace(r'\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing and Lemmatization\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_output = ' '.join(lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text))\n",
    "    return lemmatized_output\n",
    "\n",
    "\n",
    "svm_df.fillna('', inplace=True) \n",
    "\n",
    "svm_df['text_lemmatized'] = svm_df.content_without_stopwords.apply(lemmatize_text)\n",
    "\n",
    "#convert tweet into lower case\n",
    "svm_df['text_lemmatized'] = [entry.lower() for entry in svm_df['text_lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4400864"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop unnecessary columns\n",
    "\n",
    "svm_df = svm_df[['text_lemmatized', 'russian_troll']]\n",
    "\n",
    "svm_df.head()\n",
    "\n",
    "len(svm_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:: Sat Dec  7 23:12:11 2019\n",
      "Naive Bayes Accuracy Score ->  92.06822898519988\n",
      "Naive Bayes Precision-Recall Score ->  0.8940514426174226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  94.39322557678032\n",
      "SVM Precision Score ->  0.9106089871085874\n",
      "End time::Sat Dec  7 23:30:01 2019\n"
     ]
    }
   ],
   "source": [
    "#Training without CV\n",
    "\n",
    "print('Start time:: '+ time.asctime( time.localtime(time.time())))\n",
    "\n",
    "#Split data into training and test set\n",
    "\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(svm_df['text_lemmatized'],svm_df['russian_troll'],test_size=0.3)\n",
    "\n",
    "#Word Vectorization with TF-IDF\n",
    "Tfidf_vect = TfidfVectorizer(min_df=1, sublinear_tf = True, use_idf = True, ngram_range=(1, 2))\n",
    "Tfidf_vect.fit(svm_df['text_lemmatized'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "\n",
    "#Use ML Algorithms to predict outcome\n",
    "\n",
    "#Naive Bayes\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "average_precision_NB = average_precision_score(Test_Y, predictions_NB)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "print(\"Naive Bayes Precision-Recall Score -> \", average_precision_NB)\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.LinearSVC(C=1.0)\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "average_precision_SVM = average_precision_score(Test_Y, predictions_SVM)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Precision Score -> \", average_precision_SVM)\n",
    "\n",
    "print('End time::'+time.asctime( time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:: Thu Dec  5 22:39:36 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  1  = 0.897736129447132\n",
      "Precision of Naive fold  1  =  1.0\n",
      "Accuracy of SVM fold  1  = 0.9300479223426277\n",
      "Precision of SVM fold  1  =  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  2  = 0.8750110773551594\n",
      "Precision of Naive fold  2  =  1.0\n",
      "Accuracy of SVM fold  2  = 0.9183116065687011\n",
      "Precision of SVM fold  2  =  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  3  = 0.8990472338423994\n",
      "Precision of Naive fold  3  =  1.0\n",
      "Accuracy of SVM fold  3  = 0.9375737070170216\n",
      "Precision of SVM fold  3  =  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  4  = 0.6701811232778974\n",
      "Precision of Naive fold  4  =  1.0\n",
      "Accuracy of SVM fold  4  = 0.697739310636306\n",
      "Precision of SVM fold  4  =  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  5  = 0.8715569229650568\n",
      "Precision of Naive fold  5  =  0.9297868664018728\n",
      "Accuracy of SVM fold  5  = 0.9030644010488859\n",
      "Precision of SVM fold  5  =  0.9458767319454435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  6  = 0.8680439732234154\n",
      "Precision of Naive fold  6  =  nan\n",
      "Accuracy of SVM fold  6  = 0.9158096372072732\n",
      "Precision of SVM fold  6  =  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  7  = 0.8878128365819408\n",
      "Precision of Naive fold  7  =  nan\n",
      "Accuracy of SVM fold  7  = 0.9257940493449007\n",
      "Precision of SVM fold  7  =  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  8  = 0.8907599878205623\n",
      "Precision of Naive fold  8  =  nan\n",
      "Accuracy of SVM fold  8  = 0.8966792854123967\n",
      "Precision of SVM fold  8  =  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  9  = 0.9582513417831969\n",
      "Precision of Naive fold  9  =  nan\n",
      "Accuracy of SVM fold  9  = 0.9619801584235809\n",
      "Precision of SVM fold  9  =  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive fold  10  = 0.9562221929350172\n",
      "Precision of Naive fold  10  =  nan\n",
      "Accuracy of SVM fold  10  = 0.9608031157546479\n",
      "Precision of SVM fold  10  =  nan\n",
      "Average Naive accuracy across 10 folds=  0.8774622819231779\n",
      "Average SVM accuracy across 10 folds=  0.9047803193756341\n",
      "Average Naive precision across 10 folds =  nan\n",
      "Average SVM precision across 10 folds =  nan\n",
      "End time::Fri Dec  6 01:30:53 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithra Menon\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "#K-Fold CV with SVM\n",
    "\n",
    "print('Start time:: '+ time.asctime( time.localtime(time.time())))\n",
    "\n",
    "#initialize Vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(sublinear_tf = True, use_idf = True, ngram_range=(1, 2))\n",
    "Tfidf_vect.fit(svm_df['text_lemmatized'])\n",
    "\n",
    "#Initialize models\n",
    "model_naive = naive_bayes.MultinomialNB()\n",
    "model_svm = svm.LinearSVC(C=1.0)\n",
    "\n",
    "#Rename for simplicity\n",
    "X = svm_df['text_lemmatized']\n",
    "y = svm_df['russian_troll']\n",
    "\n",
    "#initialize K-Fold to 10\n",
    "kf = model_selection.KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "#initialize temporary variables\n",
    "sum_acc_svm = 0\n",
    "sum_acc_naive = 0\n",
    "sum_prec_svm = 0\n",
    "sum_prec_naive = 0\n",
    "i = 0\n",
    "\n",
    "#split the data and CV\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    #split train-test model\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    #Vectorize and transform the data\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "    \n",
    "    Train_X_Tfidf.shape\n",
    "    Test_X_Tfidf.shape\n",
    "    \n",
    "    #Fit the models\n",
    "    model_svm.fit(Train_X_Tfidf, y_train)\n",
    "    model_naive.fit(Train_X_Tfidf, y_train)\n",
    "    \n",
    "    #Predict on test data\n",
    "    y_predict_svm = model_svm.predict(Test_X_Tfidf)\n",
    "    y_predict_naive = model_naive.predict(Test_X_Tfidf)\n",
    "      \n",
    "    #Calculate accuracy of models    \n",
    "    accuracy_svm = accuracy_score(y_test, y_predict_svm) \n",
    "    accuracy_naive = accuracy_score(y_test, y_predict_naive) \n",
    "    \n",
    "    #Calculate Precision-Recall score of models\n",
    "    precision_naive = average_precision_score(y_test, y_predict_naive)\n",
    "    precision_svm = average_precision_score(y_test, y_predict_svm)\n",
    "    \n",
    "    #Add the values into temporary variable to calculate average\n",
    "    sum_acc_svm += accuracy_svm\n",
    "    sum_acc_naive += accuracy_naive\n",
    "    sum_prec_svm += precision_svm\n",
    "    sum_prec_naive += precision_naive\n",
    "    i += 1\n",
    "    \n",
    "    #Print calculated values for each fold\n",
    "    print(\"Accuracy of Naive fold \",i,\" =\", accuracy_naive)\n",
    "    print(\"Precision of Naive fold \",i, \" = \", precision_naive)\n",
    "    print(\"Accuracy of SVM fold \",i,\" =\", accuracy_svm)\n",
    "    print(\"Precision of SVM fold \",i, \" = \", precision_svm)\n",
    "    \n",
    "\n",
    "#calculate mean of all evaluation parameters\n",
    "mean_accuracy_svm = sum_acc_svm/10   \n",
    "mean_accuracy_naive = sum_acc_naive/10  \n",
    "mean_precision_svm = sum_prec_svm/10\n",
    "mean_precision_naive = sum_prec_naive/10\n",
    "\n",
    "#Print avergae values across 10 Folds\n",
    "print(\"Average Naive accuracy across 10 folds= \", mean_accuracy_naive)    \n",
    "print(\"Average SVM accuracy across 10 folds= \", mean_accuracy_svm)   \n",
    "print(\"Average Naive precision across 10 folds = \", mean_precision_naive)\n",
    "print(\"Average SVM precision across 10 folds = \", mean_precision_svm)\n",
    "\n",
    "print('End time::'+time.asctime( time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1]\n",
      "   0\n",
      "0  0\n",
      "1  1\n",
      "2  0\n",
      "3  1\n"
     ]
    }
   ],
   "source": [
    "#Real time prediction\n",
    "real_tweets = pd.read_csv(r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls\\RealTweets.csv')\n",
    "\n",
    "#Preprocessing the tweet content in real_tweets\n",
    "\n",
    "#remove URLs from the tweet content\n",
    "real_tweets['content'] = real_tweets['content'].str.replace('https?:\\/\\/.*[\\r\\n]*', '')\n",
    "\n",
    "#remove non ASCII characters from tweets\n",
    "real_tweets['content'] = real_tweets['content'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "#remove words starting with @username as its not relevant to our classification\n",
    "real_tweets['content'] = real_tweets['content'].str.replace('@(.+?)[\\s,.;]', '')\n",
    "\n",
    "#remove words starting with & as they represent HTML character reference \n",
    "real_tweets['content'] = real_tweets['content'].str.replace('&(.+?)[\\s,.;]', '')\n",
    "\n",
    "#remove numerics and special characters except # from the string\n",
    "real_tweets['content'] = real_tweets['content'].str.replace('[^a-zA-Z#\\s]', '')\n",
    "\n",
    "\n",
    "#Remove stopwords from svm_df\n",
    "real_tweets['content_without_stopwords'] = real_tweets['content'].str.replace(pat, '')\n",
    "real_tweets['content_without_stopwords'] = real_tweets['content_without_stopwords'].str.replace(r'\\s+', ' ')\n",
    "\n",
    "#Tokenizing and Lemmatization\n",
    "real_tweets.fillna('', inplace=True) \n",
    "\n",
    "real_tweets['text_lemmatized'] = real_tweets.content_without_stopwords.apply(lemmatize_text)\n",
    "\n",
    "#convert tweet into lower case\n",
    "real_tweets['text_lemmatized'] = [entry.lower() for entry in real_tweets['text_lemmatized']]\n",
    "\n",
    "#Drop unnecessary columns\n",
    "real_tweets = real_tweets[['text_lemmatized']]\n",
    "\n",
    "#Word Vectorization with TF-IDF\n",
    "Real_X_Tfidf = Tfidf_vect.transform(real_tweets['text_lemmatized'])\n",
    "\n",
    "#Predict outcome\n",
    "predictions_SVM = SVM.predict(Real_X_Tfidf)\n",
    "\n",
    "#print outcomes\n",
    "print(predictions_SVM)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
