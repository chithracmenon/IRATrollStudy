{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import time\n",
    "import nltk\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API keysconsumer_key = 'rqPOoqdCS3qpmlpzfuaGVFvZ1'\n",
    "consumer_secret ='***InsertKeys***'\n",
    "access_token = '***InsertKeys***'\n",
    "access_token_secret = '***InsertKeys***'\n",
    "\n",
    "# OAuth process, using the keys and tokens\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    " \n",
    "# Creation of the actual interface, using authentication\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stream Listener to get random tweets\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):        \n",
    "        with open('fetched_tweets.json','a', encoding=\"utf8\") as tf:\n",
    "            tf.write(status.text)\n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            return False\n",
    "        \n",
    "\n",
    "#Get tweets in english from US\n",
    "stream_listener = StreamListener()\n",
    "stream = tweepy.Stream(auth=api.auth, listener=stream_listener)\n",
    "stream.filter(languages=[\"en\"], locations = [-122.75,36.8,-121.75,37.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real time prediction for fetched tweets done with model trained in RussianTrolls.ipynb\n",
    "real_tweets = pd.read_csv(r'C:\\Users\\Chithra Menon\\Downloads\\RussianTrolls\\RealTweets.csv')\n",
    "\n",
    "#Preprocessing the tweet content in real_tweets\n",
    "\n",
    "#remove URLs from the tweet content\n",
    "real_tweets['content'] = real_tweets['content'].str.replace('https?:\\/\\/.*[\\r\\n]*', '')\n",
    "\n",
    "#remove non ASCII characters from tweets\n",
    "real_tweets['content'] = real_tweets['content'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "#remove words starting with @username as its not relevant to our classification\n",
    "real_tweets['content'] = real_tweets['content'].str.replace('@(.+?)[\\s,.;]', '')\n",
    "\n",
    "#remove words starting with & as they represent HTML character reference \n",
    "real_tweets['content'] = real_tweets['content'].str.replace('&(.+?)[\\s,.;]', '')\n",
    "\n",
    "#remove numerics and special characters except # from the string\n",
    "real_tweets['content'] = real_tweets['content'].str.replace('[^a-zA-Z#\\s]', '')\n",
    "\n",
    "\n",
    "#Remove stopwords from svm_df\n",
    "real_tweets['content_without_stopwords'] = real_tweets['content'].str.replace(pat, '')\n",
    "real_tweets['content_without_stopwords'] = real_tweets['content_without_stopwords'].str.replace(r'\\s+', ' ')\n",
    "\n",
    "#Tokenizing and Lemmatization\n",
    "real_tweets.fillna('', inplace=True) \n",
    "\n",
    "real_tweets['text_lemmatized'] = real_tweets.content_without_stopwords.apply(lemmatize_text)\n",
    "\n",
    "#convert tweet into lower case\n",
    "real_tweets['text_lemmatized'] = [entry.lower() for entry in real_tweets['text_lemmatized']]\n",
    "\n",
    "#Drop unnecessary columns\n",
    "real_tweets = real_tweets[['text_lemmatized']]\n",
    "\n",
    "#Word Vectorization with TF-IDF\n",
    "Real_X_Tfidf = Tfidf_vect.transform(real_tweets['text_lemmatized'])\n",
    "\n",
    "#Predict outcome\n",
    "predictions_SVM = SVM.predict(Real_X_Tfidf)\n",
    "\n",
    "#print outcomes\n",
    "print(predictions_SVM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
